Supervised Machine Learning:

Introduction to Supervised Machine learning:
	Review of Important Terms:
		1.Feature representation
		2.Data Instances 
		3.Target Value
		4.Training and Test sets
		5.Model/Estimator
		6.Evaluation method	

	Classification and Regression:
		1. Both classification and regression take a set of training instances and learn a mapping to a target value
		2. For Classification, the target value is a discrete class value
			-Binary 
			-Multi-class: target value is one of a set of discrete values
			-Multi-label: there are multiple target value(labels)

	Supervised learning methods:
		K-nearest neighbors: makes few assumptions about the structure of the data and gives potentially accurate but sometimes
				     unstatble predictions(sensitive to small changes in the training data)
		Linear models: makes strong assumptions about the structure of the data and give stable but potentially inaccurate predictions.
		
	随着training的classifier越来越细，training set score会越来越高，test set score会逐渐上升到达峰值后会开始降低


Overfitting and Underfitting:
	Generalization, Overfitting and Underfitting:
		- Generalization ability refers to an algorithm's ability to give acccurate predictions for new, previous unseen data
		- Overfitting: Models that are too complex for the training data, are said to overfit and are not likely to generalize well to new examples.
		- Underfitting: Models that are too simple, that don't do well on training data, are said to underfit.	


Supervised Learning: Datasets:
	数据预处理（归一化，标准化，正则化）
		-标准化：公式为：(X-mean)/std  计算时对每个属性/每列分别进行，使用sklearn.preprocessing.scale()函数，可以直接将给定数据进行标准化。
		-归一化：除了上述介绍的方法之外，另一种常用的方法是将属性缩放到一个指定的最大和最小值（通常是1-0）之间，这可以通过preprocessing.MinMaxScaler类实现。

			使用这种方法的目的包括：

				1、对于方差非常小的属性可以增强其稳定性。

				2、维持稀疏矩阵中为0的条目。	
		-正则化：正则化的过程是将每个样本缩放到单位范数（每个样本的范数为1），如果后面要使用如二次型（点积）或者其它核方法计算两个样本之间的相似性这个方法会很有用。	

	Refer: https://www.cnblogs.com/charlotte77/p/5622325.html


K-Nearest Neighbors: Classification and Regression
	Regression:
		from sklearn.neighbors import KNeighborsRegressor
		X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1, random_state = 0)
		knnreg = KNeighborsRegressor(n_neighbors = 5).fit(X_train, y_train)

		print(knnreg.predict(X_test))
		print('R-squared test score: {:.3f}'
     			.format(knnreg.score(X_test, y_test)))
	Classification:
		from adspy_shared_utilities import plot_two_class_knn

		X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2,
                                                   random_state=0)

		plot_two_class_knn(X_train, y_train, 1, 'uniform', X_test, y_test)
		plot_two_class_knn(X_train, y_train, 3, 'uniform', X_test, y_test)
		plot_two_class_knn(X_train, y_train, 11, 'uniform', X_test, y_test)

	
	The r-squre regression score:
		- Measure how well a prediction model for regression fits the given data 
		- Then Score is between 0 and 1(越靠近1 model越接近完美)
		- Also known as 'Coefficient of dertermination'
	!!!A k-nearest neighbor approach can be a reasonable baseline against what you can compare more sophisticated methods. 
	if your data is sparse,do not use knn.



Linear Regression: Least-Square:
	Linear Model:		
		A linear model is a sum of weighted variables that predicts a target output value given an input data instance.  
		^y = ^w0x0+^b (^w0 slope, ^b y-intercept)
		
	Least-squares Linear Regression("Ordinary least-squares"):
		- Find w and b that minimizes the mean squared error of the model(RSS ): sum((actual value-predict value)^2). 
		- No parameters to control model complexity.


	How are linear regression parameters w,b estimated?
		- Parameters are estimated from training data
		- There are many different way to estimate w and b
		- The learning algorithm finds the parameters that optimize an objective function, typically to minimize some kind 
		 of loss function of the predicted target values vs. actural target values.
		 (loss function = a penalty value for incorrect predictions)

	Code: 
		from sklearn.linear_model import LinearRegression
		X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1,
                                                   random_state = 0)
		linreg = LinearRegression().fit(X_train, y_train)

		print('linear model coeff (w): {}'
     			.format(linreg.coef_))
		print('linear model intercept (b): {:.3f}'
     			.format(linreg.intercept_))
		print('R-squared score (training): {:.3f}'
     			.format(linreg.score(X_train, y_train)))
		print('R-squared score (test): {:.3f}'
    			 .format(linreg.score(X_test, y_test)))


Linear Regression: Ridge, Lasso, and polynomial Regression(中文讲解链接：https://blog.csdn.net/sinat_26917383/article/details/52092040):
	第一项L为训练误差Loss function，第二项为正则化项（惩罚项）越大说明模型越简单。第一项是为了最小化训练误差，得到最好的拟合数据，；
	第二项是为了简化模型，防止过拟合，得到更好的泛化能力。	
	L1：向量中各元素绝对值的和。作用是产生少量的特征，而其他的特征都是0，常用于特征选择； 
	L2：向量中各个元素平方之和再开根号。作用是选择较多的特征，使他们都趋近于0；
	Ridge Regression(L2 penalty): 
		-Use same least-square criterion, but add a penalty for large variations in w parameters(加入了w^2参数的和).
		-The addition of parameter penalty is called regularization, which prevents overfitting by restricting the model, typically to reduce it complexity.
		-Higher alpha means more regularization and simpler models.
		-Minimize the sum of squares of coefficients entries

		Code(need feature processing and normalization):
			from sklearn.linear_model import Ridge
			X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,
                                                   random_state = 0)
			linridge = Ridge(alpha=20.0).fit(X_train, y_train)
	
	The need for feature Normalization:
		-Important for some machine larning methods taht all features are on the same scale(faster convergence in learning)
		- MinMax scaling: x_new = (x_old-x_MIN)/(x_MAX-x_MIN)
			Code: 
				from sklearn.preprocessing import MinMaxScaler
				scaler = MinMaxScaler()
				X_train_scaled = scaler.fit_transform(X_train)要先fit.transform(partData),再transform(restData)不然会报错
				X_test_scaled = scaler.transform(X_test)先scaler training set，再用这个transform test set
				(用不同的scaler去train 训练和测试数据集，lead to random skew in the data)

	
	Lasso Regression:(L1 penalty):
		- Has the effect of setting parameter weights in w to zero for the least influential variables. 
		  Called sparse solution, a kind of feature selection
		- Minimize the sum of the absolute value of the coefficients
		
	!!!!!!!!!When to use rridge vs lasso regression:
		- Many small/medium sized effects: use ridge.
		- Only a few variables with medium/large effect: use lasso. 
		
	
	Polynomial Feature with Linear Regression(can be a curve):
		- Generate new features consisting of all polynomial combinations of the original two features(x_0, x_1) into x_0*x_1
		- This is still a weighted linear combination of features, so it is still a linear model, can use least-squares estimition method as well.

		Code:	X_F1_poly = poly.fit_transform(X_F1)

			X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1,
                                                   random_state = 0)
			linreg = LinearRegression().fit(X_train, y_train)


Logistic Regression(用于处理0，1分类的问题，因为正常线性回归无法做到sigmoid的非线性形式分类):
	- It can be used in the cases where the target value to be predicted is multi class catgorical variable, not just binary.
	- The logistic function transforms real-valued input to an output number y between 0 and 1, interpreted as the probability the input object belongs to the positive class.
	- C值越小，正则强度越大
	Regularization:
		- L2 regularization is on by default(like ridge)
		- parameter C control amount of regularization(default 1.0)
		- As with regularized lineare regression, it can be important to normalized all features so that they are on the same scale. 


	

Linear Classifiers: Support Vector Machines(Binary Classification双标签): 
	f(x,w,b) = sign(w・x+b) = w_1*x_1+w_2*x_2
	Classfier Margin: Defined as the maximum width the decision boundary are can be increased before hitting a nearest data point.
	The linear classifier with max margin is a linear Support Vector Machine(LSVM).
	
	Linear Support Vector Machine:
		Code:
			from sklearn.svm import SVC
			from adspy_shared_utilities import plot_class_regions_for_classifier_subplot
	
			X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state = 0)	

			fig, subaxes = plt.subplots(1, 1, figsize=(7, 5))
			this_C = 1.0
			clf = SVC(kernel = 'linear', C=this_C).fit(X_train, y_train)
			title = 'Linear SVC, C = {:.3f}'.format(this_C)
			plot_class_regions_for_classifier_subplot(clf, X_train, y_train, None, None, title, subaxes)
	
	
	Regularization for SVMs: the C parameter:
		-The strength of regularization is deterrmined by C
		-Larger values of C: less regularization
			-fit the training data as well as possible 
		-Smaller values of C: more regularization
			-More tolerant of errors on individual data points
	

	Linear Model pros and cons:
		Pros: Simple and easy to train; Fast Prediction; Works well with sparse data
		Cons: For lower - dimensional data, other models may have superior generalizaion performance;
		      For classfication, data may not be linearly separable. 


Multi-Class Classification(多标签):
	Code: from sklearn.svm import LinearSVC

		X_train, X_test, y_train, y_test = train_test_split(X_fruits_2d, y_fruits_2d, random_state = 0)

		clf = LinearSVC(C=5, random_state = 67).fit(X_train, y_train)
		print('Coefficients:\n', clf.coef_)
		print('Intercepts:\n', clf.intercept_)会出现四条linear，因为是四个labels，会apple vs rest fruit; orange vs rest fruit.....

	
	
Kernelized Support Vector Machines(Can be used in both regression and classification):
	- Tansforming the data can make it much easier for linear classifier, set the low-dimension to high-demension. 

	Radial Basis Function Kernel(径向基核函数):
		Gamma Parameter: kernel width parameter(小对应square distance很长)

		Code：	# The default SVC kernel is radial basis function (RBF)
			plot_class_regions_for_classifier(SVC().fit(X_train, y_train),
                                 X_train, y_train, None, None,
                                 'Support Vector Classifier: RBF kernel')
	Pros and Cons:
		Pro: different kernel functions can be specified, or custom kernels can be defined for specific data types.
		     Works well for both low and high-dimensional data.
		Cons: Efficiency decrease as training set size increases;
		      Needs careful normalization of input data and parameter tuning;
		      No probalility estimates.
	
	Model Complexity: 
		Kernel: type of kernel function to be used(default 'rbf', other include 'polynomial')	
		Kernel parameters: gamma
		C: regularization parameter
		Typically C and gamma are tunned at the same time. 


Cross-Validation: 
	Code: cv_scores = cross_val_score(clf, X, y)
	Stratified Cross-Validation: stratified folds each contain a proportion of classes that matches the overall dataset. Now, all classes will be fairly represented in the test set. 
	1).K-fold Cross Validation(记为K-CV)
	将原始数据分成K组(一般是均分),将每个子集数据分别做一次验证集,其余的K-1组子集数据作为训练集,这样会得到K个模型,
	用这K个模型最终的验证集的分类准确率的平均数作为此K-CV下分类器的性能指标.K一般大于等于2,实际操作时一般从3开始取,
	只有在原始数据集合数据量小的时候才会尝试取2.K-CV可以有效的避免过学习以及欠学习状态的发生,最后得到的结果也比较具有说服性.
	2).Leave-One-Out Cross Validation(记为LOO-CV)
	如果设原始数据有N个样本,那么LOO-CV就是N-CV,即每个样本单独作为验证集,其余的N-1个样本作为训练集,所以LOO-CV会得到N个模型,
	用这N个模型最终的验证集的分类准确率的平均数作为此下LOO-CV分类器的性能指标.相比于前面的K-CV,LOO-CV有两个明显的优点:
		a.每一回合中几乎所有的样本皆用于训练模型,因此最接近原始样本的分布,这样评估所得的结果比较可靠。
		b.实验过程中没有随机因素会影响实验数据,确保实验过程是可以被复制的。
	K-fold是把数据分开，leave-one-out是把每个样本分出来。


Decision Tree：
	Code: from sklearn.tree import DecisionTreeClassifier	
	      clf = DecisionTreeClassifier().fit(X_train, y_train)
	      plot_decision_tree(clf, iris.feature_names, iris.target_names)(可视化)

	Controlling the Model Complexity of Decision Trees(Prevent Overfitting):
		pre-pruning(Sklearn only have pre-prunning): 
			max_depth: number of split points
			max _leaf:
			min_sample_leaf: threshold for the minimum # of data instances a leaf can have to avoid further splitting(最后一层的node数，越小说明模型越拟合)
			max_leaf_nodes: limits total number of leaves in the tree.
		post-pruning:
		
	
	Feature Importance: 
		Feature importance of 0 ==> the feature was not used in prediction
		Feature importance of 1 ==> the feature predicts the target perfectly
		All feature importances are normalized to sum to 1
	
	
	Decision Tree: Pros and Cons:
		Pros: Easily visulized and interpreted; 
		      No feature normalization or scaling needed; 
                      Work well with dataset using a mixture of feature types.
		Cons: Even after tunning, decision tree can still overfit; 
                      Usually need an ensemble of trees for better generralization performance. 
	
		








