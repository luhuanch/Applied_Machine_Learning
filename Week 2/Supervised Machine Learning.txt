Supervised Machine Learning:

Introduction to Supervised Machine learning:
	Review of Important Terms:
		1.Feature representation
		2.Data Instances 
		3.Target Value
		4.Training and Test sets
		5.Model/Estimator
		6.Evaluation method	

	Classification and Regression:
		1. Both classification and regression take a set of training instances and learn a mapping to a target value
		2. For Classification, the target value is a discrete class value
			-Binary 
			-Multi-class: target value is one of a set of discrete values
			-Multi-label: there are multiple target value(labels)

	Supervised learning methods:
		K-nearest neighbors: makes few assumptions about the structure of the data and gives potentially accurate but sometimes
				     unstatble predictions(sensitive to small changes in the training data)
		Linear models: makes strong assumptions about the structure of the data and give stable but potentially inaccurate predictions.
		
	随着training的classifier越来越细，training set score会越来越高，test set score会逐渐上升到达峰值后会开始降低


Overfitting and Underfitting:
	Generalization, Overfitting and Underfitting:
		- Generalization ability refers to an algorithm's ability to give acccurate predictions for new, previous unseen data
		- Overfitting: Models that are too complex for the training data, are said to overfit and are not likely to generalize well to new examples.
		- Underfitting: Models that are too simple, that don't do well on training data, are said to underfit.	


Supervised Learning: Datasets:
	数据预处理（归一化，标准化，正则化）
		-标准化：公式为：(X-mean)/std  计算时对每个属性/每列分别进行，使用sklearn.preprocessing.scale()函数，可以直接将给定数据进行标准化。
		-归一化：除了上述介绍的方法之外，另一种常用的方法是将属性缩放到一个指定的最大和最小值（通常是1-0）之间，这可以通过preprocessing.MinMaxScaler类实现。

			使用这种方法的目的包括：

				1、对于方差非常小的属性可以增强其稳定性。

				2、维持稀疏矩阵中为0的条目。	
		-正则化：正则化的过程是将每个样本缩放到单位范数（每个样本的范数为1），如果后面要使用如二次型（点积）或者其它核方法计算两个样本之间的相似性这个方法会很有用。	

	Refer: https://www.cnblogs.com/charlotte77/p/5622325.html


K-Nearest Neighbors: Classification and Regression
	Regression:
		from sklearn.neighbors import KNeighborsRegressor
		X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1, random_state = 0)
		knnreg = KNeighborsRegressor(n_neighbors = 5).fit(X_train, y_train)

		print(knnreg.predict(X_test))
		print('R-squared test score: {:.3f}'
     			.format(knnreg.score(X_test, y_test)))
	Classification:
		from adspy_shared_utilities import plot_two_class_knn

		X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2,
                                                   random_state=0)

		plot_two_class_knn(X_train, y_train, 1, 'uniform', X_test, y_test)
		plot_two_class_knn(X_train, y_train, 3, 'uniform', X_test, y_test)
		plot_two_class_knn(X_train, y_train, 11, 'uniform', X_test, y_test)

	
	The r-squre regression score:
		- Measure how well a prediction model for regression fits the given data 
		- Then Score is between 0 and 1(越靠近1 model越接近完美)
		- Also known as 'Coefficient of dertermination'
	!!!A k-nearest neighbor approach can be a reasonable baseline against what you can compare more sophisticated methods. 
	if your data is sparse,do not use knn.



Linear Regression: Least-Square:
	 Linear Model:		
		A linear model is a sum of weighted variables that predicts a target output value given an input data instance.  
		^y = ^w0x0+^b (^w0 slope, ^b y-intercept)
		
	Least-squares Linear Regression("Ordinary least-squares"):
		- Find w and b that minimizes the mean squared error of the model(RSS ): sum((actual value-predict value)^2). 
		- No parameters to control model complexity.


	How are linear regression parameters w,b estimated?
		- Parameters are estimated from training data
		- There are many different way to estimate w and b
		-The learning algorithm finds the parameters that optimize an objective function, typically to minimize some kind 
		 of loss function of the predicted target values vs. actural target values.
		 (loss function = a penalty value for incorrect predictions)

	Code: 
		from sklearn.linear_model import LinearRegression
		X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1,
                                                   random_state = 0)
		linreg = LinearRegression().fit(X_train, y_train)

		print('linear model coeff (w): {}'
     			.format(linreg.coef_))
		print('linear model intercept (b): {:.3f}'
     			.format(linreg.intercept_))
		print('R-squared score (training): {:.3f}'
     			.format(linreg.score(X_train, y_train)))
		print('R-squared score (test): {:.3f}'
    			 .format(linreg.score(X_test, y_test)))


Linear Regression: Ridge, Lasso, and polynomial Regression(中文讲解链接：https://blog.csdn.net/sinat_26917383/article/details/52092040):
	第一项L为训练误差Loss function，第二项为正则化项（惩罚项）。第一项是为了最小化训练误差，得到最好的拟合数据，；
	第二项是为了简化模型，防止过拟合，得到更好的泛化能力。	
	L1：向量中各元素绝对值的和。作用是产生少量的特征，而其他的特征都是0，常用于特征选择； 
	L2：向量中各个元素平方之和再开根号。作用是选择较多的特征，使他们都趋近于0；
	Ridge Regression(L2 penalty): 
		-Use same least-square criterion, but add a penalty for large variations in w parameters(加入了w^2参数的和).
		-The addition of parameter penalty is called regularization, which prevents overfitting by restricting the model, typically to reduce it complexity.
		-Higher alpha means more regularization and simpler models.
		-Minimize the sum of squares of coefficients entries

		Code(need feature processing and normalization):
			from sklearn.linear_model import Ridge
			X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,
                                                   random_state = 0)
			linridge = Ridge(alpha=20.0).fit(X_train, y_train)
	
	The need for feature Normalization:
		-Important for some machine larning methods taht all features are on the same scale(faster convergence in learning)
		- MinMax scaling: x_new = (x_old-x_MIN)/(x_MAX-x_MIN)
			Code: 
				from sklearn.preprocessing import MinMaxScaler
				scaler = MinMaxScaler()
				X_train_scaled = scaler.fit_transform(X_train)要先fit.transform(partData),再transform(restData)不然会报错
				X_test_scaled = scaler.transform(X_test)先scaler training set，再用这个transform test set
				(用不同的scaler去train 训练和测试数据集，lead to random skew in the data)

	
	Lasso Regression:(L1 penalty):
		- Has the effect of setting parameter weights in w to zero for the least influential variables. 
		  Called sparse solution, a kind of feature selection
		- Minimize the sum of the absolute value of the coefficients
		
	!!!!!!!!!When to use rridge vs lasso regression:
		- Many small/medium sized effects: use ridge.
		- Only a few variables with medium/large effect: use lasso. 
		
	
	Polynomial Feature with Linear Regression(can be a curve):
		- Generate new features consisting of all polynomial combinations of the original two features(x_0, x_1) into x_0*x_1
		- This is still a weighted linear combination of features, so it is still a linear model, can use least-squares estimition method as well.

		Code:	X_F1_poly = poly.fit_transform(X_F1)

			X_train, X_test, y_train, y_test = train_test_split(X_F1_poly, y_F1,
                                                   random_state = 0)
			linreg = LinearRegression().fit(X_train, y_train)


Logistic Regression:
	- It can be used in the cases where the target value to be predicted is multi class catgorical variable, not just binary.
	- The logistic function transforms real-valued input to an output number y between 0 and 1, interpreted as the probability the input object belongs to the positive class.
	- C值越小，正则强度越大
	Regularization:
		- L2 regularization is on by default(like ridge)
		- parameter C control amount of regularization(default 1.0)
		- As with regularized lineare regression, it can be important to normalized all features so that they are on the same scale. 


	
	





